---
title: "Capstone Project Pre-Processing and Feature Engineering"
author: "Steve Uzupis"
date: "2024-07-07"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

$$Enviroment Build$$

*Packages Load*
```{r package load: su}
# load packages

library(readr)         # load csv files
#library(readxl)        # load excel files
library(dplyr)         # data manipulation
#library(lubridate)     # date & time manipulation
library(ggplot2)       # data visualization
#library(tidyr)         # collection of statistical packages, packages loaded individually
#library(corrplot)      # to visualize correlations
#library(leaps)         # for subset selection
library(caret)         # test for correlation and create dummy variables
#library(car)           # for VIF
#library(scales)        # for visualizing plots in %
#library(forcats)       # ordering data frames
#library(codebookr)     # adding appendix to r code
#library(gtsummary)     # creating tables
#library(cardx)         # to include statistic results
#library(moments)       # to calculate skewness and kurtosis
#library(VIM)           # to run K- Nearest Neighbour
#library(pROC)          # to analyse and display reciever operating characteristics(ROC) curvees
#library(randomForest)  # to impletement Random Forest algorithm
#library(mice)          # Imputing missing values using mice
```

*Data load*
```{r data load: su}
# load data
sdoh_data <- read_csv("data/sdoh_data.csv")
dim(sdoh_data)

# remove unwanted features, create calculated feature, convert fips_code to data type matching chr_data
sdoh_data <- sdoh_data %>% 
  select("COUNTYFIPS", 
         "STATE", 
         "COUNTY", 
         "REGION", 
         "ACS_TOT_POP_WT", 
         "ACS_AVG_HH_SIZE", 
         "ACS_PCT_MALE", 
         "ACS_PCT_AIAN", 
         "ACS_PCT_ASIAN", 
         "ACS_PCT_BLACK", 
         "ACS_PCT_HISPANIC", 
         "ACS_PCT_OTHER_RACE", 
         "ACS_PCT_WHITE", 
         "ACS_PCT_CHILD_1FAM", 
         "ACS_PCT_CHILDREN_GRANDPARENT", 
         "ACS_PCT_GRANDP_RESPS_NO_P", 
         "ACS_PCT_GRANDP_RESPS_P",
         "ACS_PCT_HH_OTHER_COMP", 
         "ACS_PCT_HH_INTERNET",
         "ACS_PCT_EMPLOYED",
         "ACS_PCT_HH_INC_99999",
         "ACS_PCT_MEDICARE_ONLY",
         "AHRF_CLIN_NURSE_SPEC_RATE", 
         "AHRF_DENTISTS_RATE",
         "AHRF_PHYSICIAN_ASSIST_RATE",
         "AMFAR_MHFAC_RATE",
         "CEN_POPDENSITY_COUNTY",
         "NEPHTN_HEATIND_90",
         "SAIPE_MEDIAN_HH_INCOME",
         "POS_MEDIAN_DIST_ED",
         "POS_MEDIAN_DIST_PED_ICU",
         "POS_MEDIAN_DIST_CLINIC", 
         "POS_MEDIAN_DIST_ALC", 
  ) %>% 
  mutate(pct_grandparents_as_guardians = ACS_PCT_CHILDREN_GRANDPARENT * ((ACS_PCT_GRANDP_RESPS_P + ACS_PCT_GRANDP_RESPS_NO_P)/100)) %>% 
  select(-ACS_PCT_GRANDP_RESPS_P, -ACS_PCT_GRANDP_RESPS_NO_P, -ACS_PCT_CHILDREN_GRANDPARENT) %>% 
  rename("fips_code" = "COUNTYFIPS",
         "state" = "STATE",
         "county" = "COUNTY",
         "region" = "REGION",
         "weighted_population"  = "ACS_TOT_POP_WT",
         "average_hh_size" = "ACS_AVG_HH_SIZE",
         "pct_male"  = "ACS_PCT_MALE",
         "pct_native_american" = "ACS_PCT_AIAN",
         "pct_asian" = "ACS_PCT_ASIAN",
         "pct_black" = "ACS_PCT_BLACK",
         "pct_hispanic" = "ACS_PCT_HISPANIC",
         "pct_other_race" = "ACS_PCT_OTHER_RACE",
         "pct_white" = "ACS_PCT_WHITE",
         "pct_single_parent" = "ACS_PCT_CHILD_1FAM",
         "pct_hh_other_computer" = "ACS_PCT_HH_OTHER_COMP",
         "pct_hh_internet" = "ACS_PCT_HH_INTERNET",
         "pct_employed" = "ACS_PCT_EMPLOYED",
         "pct_hh_inc_99999" = "ACS_PCT_HH_INC_99999",            # renamed by mg
         "pct_w_medicare" = "ACS_PCT_MEDICARE_ONLY",
         "clinical_nurse_pt" = "AHRF_CLIN_NURSE_SPEC_RATE",
         "dentist_pt" = "AHRF_DENTISTS_RATE",
         "pa_pt" = "AHRF_PHYSICIAN_ASSIST_RATE",
         "mental_health_faciliy_pt" = "AMFAR_MHFAC_RATE",
         "population_density" = "CEN_POPDENSITY_COUNTY",
         "days_over_90_f" = "NEPHTN_HEATIND_90",
         "median_hh_income" = "SAIPE_MEDIAN_HH_INCOME",
         "median_er_dist" = "POS_MEDIAN_DIST_ED",
         "median_pediatric_icu_dist" = "POS_MEDIAN_DIST_PED_ICU",
         "median_health_clinic_dist" = "POS_MEDIAN_DIST_CLINIC",
         "median_drug_alcohol_care_dist" = "POS_MEDIAN_DIST_ALC"
    
  ) %>% 
  mutate(fips_code = as.numeric(fips_code))

chr_data <- read_csv("data/chr_data.csv", skip = 1)
dim(chr_data)
# remove unwanted features
# convert principal care providers from per 100,000 people to per 1,000 people to match other data

chr_data <- chr_data %>%
  select("fipscode",
         "v002_rawvalue",
         "v009_rawvalue",
         "v011_rawvalue",
         "v070_rawvalue",  
         "v049_rawvalue",
         "v085_rawvalue",
         "v168_rawvalue", 
         "v069_rawvalue",
         "v044_rawvalue", 
         "v140_rawvalue",
         "v125_rawvalue",
         "v124_rawvalue",
         "v136_other_data_1",
         "v136_other_data_2",
         "v137_rawvalue",
         "v147_rawvalue",
         "v139_rawvalue",
         "v177_rawvalue",
         "v153_rawvalue",
         "v053_rawvalue", 
         "v058_rawvalue", 
         "v004_rawvalue",
  ) %>% 
  mutate(pcp_pt = v004_rawvalue/100) %>% 
  select(-v004_rawvalue) %>% 
  rename("fips_code" = "fipscode",
         "pct_poor_to_fair_health" = "v002_rawvalue",
         "pct_adult_smokers" = "v009_rawvalue",
         "pct_obese_adults" = "v011_rawvalue",
         "pct_no_exercise" = "v070_rawvalue",
         "pct_binge_drinkers" = "v049_rawvalue",
         "pct_under_65_no_health_insurance" = "v085_rawvalue",
         "pct_highschool_diploma" = "v168_rawvalue",
         "pct_some_college" = "v069_rawvalue",
         "inequality_ratio" = "v044_rawvalue",
         "social_clubs_per_10k" = "v140_rawvalue",
         "air_polution_metric" =  "v125_rawvalue",
         "water_quality" = "v124_rawvalue",                          # renamed by mg
         "pct_high_housing_costs" = "v136_other_data_1",
         "pct_overcrowded_hh" = "v136_other_data_2",
         "pct_food_insecurities" = "v139_rawvalue",
         "pct_voters" = "v177_rawvalue",
         "pct_home_owner" = "v153_rawvalue",
         "pct_65_plus" = "v053_rawvalue",
         "pct_rural_population" = "v058_rawvalue",
         "life_expectancy_years" = "v147_rawvalue",
         "pct_30_min_plus_commute"  = "v137_rawvalue")

# full data sets are extremely large, initial dimension reduction was performed previously
```

*Combine datasets*

```{r qol dataset create and clean: su}
# Create and clean the qol_data dataset
qol_data <- sdoh_data %>%
  inner_join(chr_data, by = "fips_code") %>%
  mutate(response = ifelse(pct_poor_to_fair_health >= 0.154, "worse", "better")) %>%
  mutate(response = as.factor(response)) %>%
  select(-pct_poor_to_fair_health) %>%              # keep until analysis has been performed
  mutate_at(vars(state, county, region), as.factor) #%>%    # convert characters to factors
#  rename("response" = "pct_poor_to_fair_health")

# clean local encviroment
rm(chr_data)
rm(sdoh_data)
```

$$Pre-processing and Feature Enginering$$

eliminate variables with no predictive value: fipscode, county and state, pct_poor_to_fair_health:
```{r no predictive value feature removal: uo}
qol_data <- qol_data %>% 
  select(
    -c(
      fips_code,
      county,
      state,
      life_expectancy_years
      )
    )
```

```{r data split: course function}
calcSplitRatio <- function(p = NA, df) {
  ## @p  = the number of parameters. by default, if none are provided, the number of columns (predictors) in the dataset are used
  ## @df = the dataframe that will be used for the analysis
  
  ## If the number of parameters isn't supplied, set it to the number of features minus 1 for the target
  if(is.na(p)) {
    p <- ncol(df) -1   ## COMMENT HERE
  }
  
  ## Calculate the ideal number of testing set
  test_N <- (1/sqrt(p))*nrow(df)
  ## Turn that into a testing proportion
  test_prop <- round((1/sqrt(p))*nrow(df)/nrow(df), 2)
  ## And find the training proportion
  train_prop <- 1-test_prop
  
  ## Tell us the results!
  print(paste0("The ideal split ratio is ", train_prop, ":", test_prop, " training:testing"))
  
  ## Return the size of the training set
  return(train_prop)
}

calcSplitRatio(df = qol_data)
```

```{r understand na distribution: uo}
# Find columns with missing data
colSums(is.na(qol_data))
```

```{r understand na distribution: su}
# sum of NAs in each column
na_counts <- colSums(is.na(qol_data))

# combine column names and NA counts into a dataframe
na_counts_df <- data.frame(variable_name = names(na_counts), na_count = na_counts)

# Sort the dataframe by NA_Count in descending order
na_counts_df <- na_counts_df[order(-na_counts_df$na_count), ]

# View the sorted dataframe
print(na_counts_df)

rm(na_counts_df)
```

```{r remove target observations with NA values: uo}
# Delete rows with missing values in the "response" column
qol_data <- qol_data[!is.na(qol_data$response), ]

# delete column with large number of missing values 'pcp_pt'
qol_data <- qol_data[, !names(qol_data) %in% "pcp_pt"]
```

```{r train and test split: uo}
# Splitting the data into train and test sets
qol_train <- qol_data[sample(nrow(qol_data), 0.86*nrow(qol_data)), ]
qol_test <- qol_data[!(rownames(qol_data) %in% rownames(qol_train)), ]
```

```{r test and train numeric features only: uo}
# Selecting only numeric columns for imputation
numeric_cols <- sapply(qol_train, is.numeric)
qol_train_numeric <- qol_train[, numeric_cols]
qol_test_numeric <- qol_test[, numeric_cols]
```

```{r impune missing data values: uo}
# Imputing missing values in the training set
qol_train_imputed <- mice::mice(
  qol_train_numeric, 
  m = 1, 
  method = "cart",  # Using CART for imputation
  maxit = 5         # Maximum iterations for imputation
  )

# Imputing missing values in the test set using the trained model
qol_test_imputed <- mice::mice(
  qol_test_numeric, 
  m = 1, 
  method = "cart",
  maxit = 5,
  use.all = FALSE   # Using only the training data for imputation
  )

# Combining the imputed numeric columns back with the original data
qol_train <- cbind(
  qol_train[, !numeric_cols],
  mice::complete(qol_train_imputed)
  )

qol_test <- cbind(
  qol_test[, !numeric_cols],
  mice::complete(qol_test_imputed)
  )

# confirm code was ran

# remove numeric list to clean environment, no further use
rm(numeric_cols)
rm(qol_train_imputed)
rm(qol_test_imputed)
rm(qol_train_numeric)
rm(qol_test_numeric)
```

```{r encode character variables: uo}
# Create dummy variables for 'region' in training data
dummies_train <- dummyVars(
  ~ region,
  data = qol_train, 
  fullRank = FALSE
  )

qol_train_encoded <- predict(
  dummies_train,
  newdata = qol_train
  )

qol_train <- cbind(
  qol_train,
  qol_train_encoded) %>%
  select(-region) # Remove the original 'region' column

# Create dummy variables for 'region' in test data
dummies_test <- dummyVars(
  ~ region,
  data = qol_test,
  fullRank = FALSE
  )

qol_test_encoded <- predict(
  dummies_test,
  newdata = qol_test
  )

qol_test <- cbind(
  qol_test,
  qol_test_encoded) %>%
  select(-region) # Remove the original 'region' column

# remove objects for cleaner enviroment
rm(dummies_train)
rm(qol_train_encoded)
rm(dummies_test)
rm(qol_test_encoded)
```

```{r encode response: uo}
#Encode response variable as 1(worse) and 0(better) 

#encode response variable in training set
qol_train <- qol_train %>%
  mutate(response = ifelse(response == "better", 0, 1))

# Convert response variable to factor 
qol_train$response <- as.factor(qol_train$response)
```

```{r create response table: uo}
# confirm response distribution is similiar between train and test
table(qol_train$response)

#encode response variable in testing set
qol_test <- qol_test %>%
  mutate(response = ifelse(response == "better", 0, 1))
table(qol_test$response)

# Convert response variable to factor 
qol_test$response <- as.factor(qol_test$response)
```

```{r missing characters after imputation: uo}
# Count missing values in the train set
total_missing <- sum(is.na(qol_train))
print(paste("Total missing values in dataframe:", total_missing))

# Count missing values in the test set
total_missing <- sum(is.na(qol_train))
print(paste("Total missing values in dataframe:", total_missing))

```

To determine if there are unwanted characters and whitespace and make corrections:
We print a sample of the data for visual inspection.
We use regular expressions to detect unwanted characters in text columns.
We check for leading, trailing, or excessive whitespace in text columns.

```{r unwanted values after imputation: uo}
# Visual inspection
#print(head(qol_train))

# Detect unwanted characters using regular expressions
unwanted_chars <- function(x) {
  if (is.character(x)) {
    return(grepl("[^[:alnum:][:space:]]", x))
  } else {
    return(FALSE)
  }
}

# Apply the function to each column qol_train
unwanted_chars_results <- sapply(qol_train, function(col) sum(unwanted_chars(col)))
print("Number of unwanted characters in each column:")
#print(unwanted_chars_results)

# Apply the function to each column qol_test
unwanted_chars_results <- sapply(qol_train, function(col) sum(unwanted_chars(col)))
print("Number of unwanted characters in each column:")
#print(unwanted_chars_results)



# Detect leading, trailing, or excessive whitespace
whitespace_check <- function(x) {
  if (is.character(x)) {
    return(grepl("^\\s+|\\s+$|\\s{2,}", x))
  } else {
    return(FALSE)
  }
}

# Apply the function to each column of training and test sets
whitespace_results <- sapply(qol_train, function(col) sum(whitespace_check(col)))
print("Number of whitespace issues in each column:")
#print(whitespace_results)


whitespace_results <- sapply(qol_test, function(col) sum(whitespace_check(col)))
print("Number of whitespace issues in each column:")
#print(whitespace_results)

```

```{r}
names(qol_train)
```



```{r scale and center: su}
# exclude response, classification variables
exclude_cols <- c(1, 48:51)  

# scale, exclude the specified columns
training_combined <- scale(qol_train[, -exclude_cols])

# get mean and sd used for scaling
train_means <- attr(training_combined, "scaled:center")
train_sds <- attr(training_combined, "scaled:scale")

# scale training set
qol_train[, -exclude_cols] <- training_combined

# scale test set predictors using training set's statistics
qol_test[, -exclude_cols] <- scale(qol_test[, -exclude_cols],
                                   center = train_means,
                                   scale = train_sds)

# response variable is continuous, scale separately
response_mean <- mean(qol_train[, 28], na.rm = TRUE)
response_sd <- sd(qol_train[, 28], na.rm = TRUE)

qol_train[, 28] <- scale(qol_train[, 28], center = response_mean, scale = response_sd)
qol_test[, 28] <- scale(qol_test[, 28], center = response_mean, scale = response_sd)

# Print the scaled data to verify
print(head(qol_train))
print(head(qol_test))


```

# PCA
```{r pca: mg}
qol_train_standardized <- qol_train %>%
  select(-response) %>%
  scale() # standardize the data (exclude the response variable)

pca_model <- prcomp(
  qol_train_standardized,
  center = TRUE,
  scale. = TRUE) # apply PCA

summary(pca_model) # print summary of PCA model

plot(pca_model,
     type = "l") # plot explained variance

pca_components <- predict(
  pca_model,
  qol_train_standardized
  ) # extract the principal components

head(pca_components)

```

```{r features to explain response variance: su}
# Calculate cumulative explained variance
explained_variance <- summary(pca_model)$importance[2, ]
cumulative_explained_variance <- cumsum(explained_variance)

# Plot cumulative explained variance
plot(cumulative_explained_variance, type = "b", xlab = "Number of Principal Components", ylab = "Cumulative Explained Variance")
abline(h = 0.95, col = "red", lty = 2) # line for 95% explained variance

# Find the number of components needed to explain 95% of the variance
num_components_95 <- which(cumulative_explained_variance >= 0.95)[1]
num_components_95

# 35 features predict 95% of response variability
```

```{r key predictors: su}
# extract the features (rotation matrix)
features <- pca_model$rotation

# subset the loadings for the first 35 principal components
top_features <- features[, 1:35]

# find the most important variables for each principal component
important_vars <- apply(top_features, 2, function(x) names(x[order(abs(x), decreasing = TRUE)]))

# convert to a data frame
important_vars_df <- as.data.frame(important_vars)
head(important_vars_df)



```


```{r key feature list format: su}
# most important variables
important_vars_list <- unique(unlist(important_vars_df))

important_vars_list

```

```{r validation: mg}
# Prepare data for cross-validation
#train_control <- trainControl(method = "cv", number = 10)
#tune_grid <- expand.grid(.ncomp = 1:ncol(pca_components))

# Train a model using cross-validation to find the optimal number of components
#cv_model <- train(
#  response ~ .,
#  data = cbind(qol_train$response, pca_components),
#  method = "lm",
#  trControl = train_control,
#  tuneGrid = tune_grid
#)

# Get the optimal number of components
#optimal_components <- cv_model$bestTune$.ncomp
#optimal_components

```

*Subset regression viability check*

Build a simple OLS model
```{r }
# --- OLS Model ---

# Fit the OLS model
ols_model <- lm(response ~ ., data = qol_train)

# Predict on the test set
predictions_ols <- predict(ols_model, newdata = qol_test)

# Evaluate the OLS model
confusion_matrix_ols <- table(qol_test$response, predictions_ols > 0.5)
accuracy_ols <- sum(diag(confusion_matrix_ols)) / sum(confusion_matrix_ols)
print(paste("Accuracy (OLS):", accuracy_ols))

# Calculate feature importance for OLS
feature_importance_ols <- abs(summary(ols_model)$coefficients[, "Estimate"])
importance_df_ols <- data.frame(Feature = names(feature_importance_ols), Importance = feature_importance_ols)
importance_df_ols <- importance_df_ols[order(-importance_df_ols$Importance), ]

# --- Random Forest Model ---

# Train a random forest model
rf_model <- randomForest::randomForest(as.factor(response) ~ ., data = qol_train, ntree = 500, na.action = na.omit)

# Predict on the test set
predictions_rf <- predict(rf_model, qol_test, type = "prob")[, 2]

# Evaluate the Random Forest model
confusion_matrix_rf <- table(qol_test$response, predictions_rf > 0.5)
accuracy_rf <- sum(diag(confusion_matrix_rf)) / sum(confusion_matrix_rf)
print(paste("Accuracy (RF):", accuracy_rf))

# --- ROC Curve (Both Models) ---

# Function to plot ROC curves
plot_roc <- function(predictions, model_name) {
  roc_obj <- roc(qol_test$response, predictions)
  auc_value <- auc(roc_obj)
  
  roc_df <- data.frame(
    tpr = roc_obj$sensitivities,
    fpr = 1 - roc_obj$specificities
  )
  
  ggplot(roc_df, aes(x = fpr, y = tpr)) +
    geom_line() +
    geom_abline(linetype = "dashed") +
    xlab("False Positive Rate") +
    ylab("True Positive Rate") +
    ggtitle(paste("ROC Curve (", model_name, ", AUC =", round(auc_value, 2), ")"))
}
```

```{r }
library(pROC)
# Plot ROC curves for both models
plot_roc(predictions_ols, "OLS")
plot_roc(predictions_rf, "Random Forest")

# --- Feature Importance Visualization (OLS) ---

# Select top 20 features
top_20_features_ols <- head(importance_df_ols, 20)

# Plot top 20 feature importance for OLS
ggplot(top_20_features_ols, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 20 Feature Importance (OLS)", x = "Feature", y = "Importance") +
  theme_minimal()

# --- Feature Importance Visualization (RF) ---

# Calculate feature importance for RF
feature_importance_rf <- randomForest::importance(rf_model)
importance_df_rf <- data.frame(Feature = rownames(feature_importance_rf), Importance = feature_importance_rf[, 1])
importance_df_rf <- importance_df_rf[order(-importance_df_rf$Importance), ]

# Select top 20 features
top_20_features_rf <- head(importance_df_rf, 20)

# Plot top 20 feature importance for RF
ggplot(top_20_features_rf, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 20 Feature Importance (RF)", x = "Feature", y = "Importance") +
  theme_minimal()

```

build  initial model (OLS)
```{r initial model: uo}
# Convert response variable to numeric
qol_train$response <- as.numeric(as.character(qol_train$response))
# Fit the OLS model
ols_model <- lm(
  response ~ .,
  data = qol_train
  )
# Predict on the test set
predictions <- predict(
  ols_model,
  newdata = qol_test
  )

# Convert predictions to binary outcomes
predicted_classes <- ifelse(predictions > 0.5, 1, 0)

# Evaluate the model
confusion_matrix <- table(
  qol_test$response,
  predicted_classes
  )

accuracy_ols <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# Extract coefficients
coefficients <- summary(ols_model)$coefficients

# Calculate importance (absolute value of coefficients)
feature_importance <- abs(coefficients[, "Estimate"])

# Create a data frame for better visualization
importance_df <- data.frame(
  Feature = rownames(coefficients),
  Importance = feature_importance
  )

# Sort by importance
importance_df <- importance_df[order(-importance_df$Importance), ]

print(importance_df)
```

visualize feature importance
```{r visualize feature importance: uo}
# Select the top 20 important features
top_20_features_ols <- importance_df %>%
  arrange(desc(Importance)) %>%
  head(20)

# Create the bar plot for the top 20 features
ggplot(top_20_features_ols, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +  # Flip coordinates for better readability
  labs(title = "Top 20 Feature Importance using ols", x = "Feature", y = "Importance") +
  theme_minimal()

```

Build ROC curve
```{r roc curve: uo}
# Load necessary libraries
library(ggplot2)
library(pROC)

# Calculate ROC curve
roc_ols <- roc(qol_test$response, predictions)

# Create a data frame for ggplot
roc_data <- data.frame(
  specificity = rev(roc_ols$specificities),
  sensitivity = rev(roc_ols$sensitivities)
)

# Plot ROC curve using ggplot2
ggplot(roc_data, aes(x = 1-specificity, y = sensitivity)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed", color = "red") +
  labs(title = paste("ROC Curve (AUC =", round(auc(roc_ols), 2), ")"),
       x = "1 - Specificity",
       y = "Sensitivity") +
  theme_minimal() +
  coord_fixed(ratio = 1)


```
### Build random forest model
```{r random forest: uo}
# Train a random forest model for classification
model_rf <- randomForest::randomForest(
  as.factor(response) ~ .,
  data = qol_train,
  ntree = 500
  )

# Predict the response variable for the testing set
predictions2 <- predict(
  model_rf,
  qol_test,
  type = "response"
  )

# Calculate the accuracy of the random forest model
accuracy <- mean(qol_test$response == predictions2)
print(accuracy)

# Calculate the ROC AUC score
roc_obj <- roc(
  qol_test$response,
  as.numeric(predictions2)
  )

auc_rf <- auc(roc_obj)
print(auc_rf)

# Plot the ROC curve using ggplot2
roc_df <- data.frame(
  tpr = roc_obj$sensitivities,
  fpr = 1 - roc_obj$specificities
)

ggplot(roc_df, aes(x = fpr, y = tpr)) +
  geom_line() +
  geom_abline(linetype = "dashed") +
  xlab("False Positive Rate") +
  ylab("True Positive Rate") +
  ggtitle(paste("ROC Curve (AUC =", round(auc_rf, 2), ")"))

# Calculate feature importance
importance_rf <- randomForest::importance(model_rf)
importance_df <- data.frame(
  Feature = rownames(importance_rf),
  Importance = importance_rf[, 1])

# Select top 20 important features
top_20_importance <- importance_df[order(-importance_df$Importance), ][1:20, ]

# Plot top 20 feature importance using ggplot2
ggplot(top_20_importance, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("Feature") +
  ylab("Importance") +
  ggtitle("Top 20 Feature Importance from Random Forest Model")

```

*Boxcox Analysis*
```{r}
qol_data <- qol_data %>%
  mutate(response = ifelse(response == "better", 1, 2)) # convert to 1 and 2 for boxcox analysis, log doesn't work on 0
```

```{r boxcox analysis table: su}
# Initialize the results data frame
transform_output <- data.frame(
  Predictor = character(),
  Optimal_Lambda = numeric(),
  Original_R_squared = numeric(),
  Original_Adj_R_squared = numeric(),
  Transformed_R_squared = numeric(),
  Transformed_Adj_R_squared = numeric(),
  stringsAsFactors = FALSE
)
```

*Repeat the following:*
```{r bc popl: su}
# Fit the initial linear model
lm_weighted_pop <- lm(
  response ~ weighted_population,
  qol_data
  )
summary(lm_weighted_pop)

# Plot diagnostic plots for the initial model
par(mfrow = c(2, 2))
plot(lm_weighted_pop)

original_r_squared <- summary(lm_weighted_pop)$r.squared
original_adj_r_squared <- summary(lm_weighted_pop)$adj.r.squared

# Perform Box-Cox transformation on the response variable
par(mfrow = c(1, 1))
box_cox_weighted_pop <- MASS::boxcox(
  object = lm_weighted_pop
  )

# identify the optimal lambda
optimal_lambda <- box_cox_weighted_pop$x[which.max(box_cox_weighted_pop$y)]
print(optimal_lambda)
```

```{r optimal lambda: su}
# add transformed data to dataframe, use log of value if optimal lambda is 0
qol_data <- qol_data %>% 
  mutate(transformed_weighted_pop = weighted_population^optimal_lambda)


# Fit a new linear model with the transformed predictor variable
transformed_lm <- lm(response ~ transformed_weighted_pop, qol_data)
summary(transformed_lm)

transformed_r_squared <- summary(transformed_lm)$r.squared
transformed_adj_r_squared <- summary(transformed_lm)$adj.r.squared

print(transformed_r_squared)
print(transformed_adj_r_squared)

# Store the results in the results data frame
results <- rbind(transform_output, data.frame(
  Predictor = "weighted_population",
  Optimal_Lambda = optimal_lambda,
  Original_R_squared = original_r_squared,
  Original_Adj_R_squared = original_adj_r_squared,
  Transformed_R_squared = transformed_r_squared,
  Transformed_Adj_R_squared = transformed_adj_r_squared
))

# Print the results
print(results)
```

*repeat*
```{r bc hh size}
# Fit the initial linear model
lm_hh_size <- lm(
  response ~ average_hh_size,
  qol_data
  )
summary(lm_hh_size)

# Plot diagnostic plots for the initial model
par(mfrow = c(2, 2))
plot(lm_hh_size)

original_r_squared2 <- summary(lm_hh_size)$r.squared
original_adj_r_squared2 <- summary(lm_hh_size)$adj.r.squared

# Perform Box-Cox transformation on the response variable
par(mfrow = c(1, 1))
box_cox_hh_size <- MASS::boxcox(
  object = lm_hh_size
  )

# identify the optimal lambda
optimal_lambda2 <- box_cox_hh_size$x[which.max(box_cox_hh_size$y)]
print(optimal_lambda2)
```

```{r }
# add transformed data to dataframe, use log of value if optimal lambda is 0
qol_data <- qol_data %>% 
  mutate(transformed_hh_size = average_hh_size^optimal_lambda2)

# Fit a new linear model with the transformed predictor variable
transformed_lm <- lm(response ~ transformed_hh_size, qol_data)
summary(transformed_lm)

transformed_r_squared2 <- summary(transformed_lm)$r.squared
transformed_adj_r_squared2 <- summary(transformed_lm)$adj.r.squared

print(transformed_r_squared2)
print(transformed_adj_r_squared2)

# Store the results in the results data frame
# Store the results in the results data frame
temp_results <- rbind(results, data.frame(
  Predictor = "average_hh_size",
  Optimal_Lambda = optimal_lambda2,
  Original_R_squared = original_r_squared2,
  Original_Adj_R_squared = original_adj_r_squared2,
  Transformed_R_squared = transformed_r_squared2,
  Transformed_Adj_R_squared = transformed_adj_r_squared2
))

results <- temp_results
# Print the results
print(results)


```

*repeat*
```{r bc male}
# Fit the initial linear model
lm_initial <- lm(
  response ~ pct_male,
  qol_data
  )
summary(lm_initial)

# Plot diagnostic plots for the initial model
par(mfrow = c(2, 2))
plot(lm_initial)

original_r_squared3 <- summary(lm_initial)$r.squared
original_adj_r_squared3 <- summary(lm_initial)$adj.r.squared

# Perform Box-Cox transformation on the response variable
par(mfrow = c(1, 1))
box_initial_lm <- MASS::boxcox(
  object = lm_initial
  )

# identify the optimal lambda
optimal_lambda3 <- box_initial_lm$x[which.max(box_initial_lm$y)]
print(optimal_lambda3)
```

```{r }
# add transformed data to dataframe, use log of value if optimal lambda is 0
qol_data <- qol_data %>% 
  mutate(transformed_male = pct_male^optimal_lambda3)

# Fit a new linear model with the transformed predictor variable
transformed_lm <- lm(response ~ transformed_male, qol_data)
summary(transformed_lm)

transformed_r_squared3 <- summary(transformed_lm)$r.squared
transformed_adj_r_squared3 <- summary(transformed_lm)$adj.r.squared

print(transformed_r_squared3)
print(transformed_adj_r_squared3)

# Store the results in the results data frame
# Store the results in the results data frame
temp_results <- rbind(results, data.frame(
  Predictor = "pct_male",
  Optimal_Lambda = optimal_lambda3,
  Original_R_squared = original_r_squared3,
  Original_Adj_R_squared = original_adj_r_squared3,
  Transformed_R_squared = transformed_r_squared3,
  Transformed_Adj_R_squared = transformed_adj_r_squared3
))

results <- temp_results
# Print the results
print(results)


```

*repeat*
```{r bc computer}
# Fit the initial linear model
lm_initial <- lm(
  response ~ pct_hh_internet,
  qol_data
  )
summary(lm_initial)

# Plot diagnostic plots for the initial model
par(mfrow = c(2, 2))
plot(lm_initial)

original_r_squared4 <- summary(lm_initial)$r.squared
original_adj_r_squared4 <- summary(lm_initial)$adj.r.squared

# Perform Box-Cox transformation on the response variable
par(mfrow = c(1, 1))
box_initial_lm <- MASS::boxcox(
  object = lm_initial
  )

# identify the optimal lambda
optimal_lambda4 <- box_initial_lm$x[which.max(box_initial_lm$y)]
print(optimal_lambda4)
```

```{r }
# add transformed data to dataframe, use log of value if optimal lambda is 0
qol_data <- qol_data %>% 
  mutate(transformed_internet = pct_hh_internet^optimal_lambda4)
```


```{r }
# Fit a new linear model with the transformed predictor variable
transformed_lm <- lm(response ~ transformed_internet, qol_data)
summary(transformed_lm)

transformed_r_squared4 <- summary(transformed_lm)$r.squared
transformed_adj_r_squared4 <- summary(transformed_lm)$adj.r.squared

print(transformed_r_squared4)
print(transformed_adj_r_squared4)

# Store the results in the results data frame
# Store the results in the results data frame
temp_results <- rbind(results, data.frame(
  Predictor = "pct_hh_internet",
  Optimal_Lambda = optimal_lambda4,
  Original_R_squared = original_r_squared4,
  Original_Adj_R_squared = original_adj_r_squared4,
  Transformed_R_squared = transformed_r_squared4,
  Transformed_Adj_R_squared = transformed_adj_r_squared4
))

results <- temp_results
# Print the results
print(results)


```

*repeat*
```{r bc employed}
# Fit the initial linear model
lm_initial <- lm(
  response ~ pct_employed,
  qol_data
  )
summary(lm_initial)

# Plot diagnostic plots for the initial model
par(mfrow = c(2, 2))
plot(lm_initial)

original_r_squared5 <- summary(lm_initial)$r.squared
original_adj_r_squared5 <- summary(lm_initial)$adj.r.squared

# Perform Box-Cox transformation on the response variable
par(mfrow = c(1, 1))
box_initial_lm <- MASS::boxcox(
  object = lm_initial
  )

# identify the optimal lambda
optimal_lambda5 <- box_initial_lm$x[which.max(box_initial_lm$y)]
print(optimal_lambda5)
```

```{r }
# add transformed data to dataframe, use log of value if optimal lambda is 0
qol_data <- qol_data %>% 
  mutate(transformed_employed = pct_employed^optimal_lambda5)
```


```{r }
# Fit a new linear model with the transformed predictor variable
transformed_lm <- lm(response ~ transformed_employed, qol_data)
summary(transformed_lm)

transformed_r_squared5 <- summary(transformed_lm)$r.squared
transformed_adj_r_squared5 <- summary(transformed_lm)$adj.r.squared

print(transformed_r_squared5)
print(transformed_adj_r_squared5)

# Store the results in the results data frame
# Store the results in the results data frame
temp_results <- rbind(results, data.frame(
  Predictor = "pct_employed",
  Optimal_Lambda = optimal_lambda5,
  Original_R_squared = original_r_squared5,
  Original_Adj_R_squared = original_adj_r_squared5,
  Transformed_R_squared = transformed_r_squared5,
  Transformed_Adj_R_squared = transformed_adj_r_squared5
))

results <- temp_results
# Print the results
print(results)


```

*repeat*
```{r bc obese}
# Fit the initial linear model
lm_initial <- lm(
  response ~ pct_obese_adults,
  qol_data
  )
summary(lm_initial)

# Plot diagnostic plots for the initial model
par(mfrow = c(2, 2))
plot(lm_initial)

original_r_squared5 <- summary(lm_initial)$r.squared
original_adj_r_squared5 <- summary(lm_initial)$adj.r.squared

# Perform Box-Cox transformation on the response variable
par(mfrow = c(1, 1))
box_initial_lm <- MASS::boxcox(
  object = lm_initial
  )

# identify the optimal lambda
optimal_lambda5 <- box_initial_lm$x[which.max(box_initial_lm$y)]
print(optimal_lambda5)
```

```{r }
# add transformed data to dataframe, use log of value if optimal lambda is 0
qol_data <- qol_data %>% 
  mutate(transformed_obese = pct_obese_adults^optimal_lambda5)
```


```{r }
# Fit a new linear model with the transformed predictor variable
transformed_lm <- lm(response ~ transformed_obese, qol_data)
summary(transformed_lm)

transformed_r_squared5 <- summary(transformed_lm)$r.squared
transformed_adj_r_squared5 <- summary(transformed_lm)$adj.r.squared

print(transformed_r_squared5)
print(transformed_adj_r_squared5)

# Store the results in the results data frame
# Store the results in the results data frame
temp_results <- rbind(results, data.frame(
  Predictor = "pct_obese_adults",
  Optimal_Lambda = optimal_lambda5,
  Original_R_squared = original_r_squared5,
  Original_Adj_R_squared = original_adj_r_squared5,
  Transformed_R_squared = transformed_r_squared5,
  Transformed_Adj_R_squared = transformed_adj_r_squared5
))

results <- temp_results
# Print the results
print(results)


```



*repeat*
```{r bc white}
# Fit the initial linear model
lm_initial <- lm(
  response ~ pct_white,
  qol_data
  )
summary(lm_initial)

# Plot diagnostic plots for the initial model
par(mfrow = c(2, 2))
plot(lm_initial)

original_r_squared6 <- summary(lm_initial)$r.squared
original_adj_r_squared6 <- summary(lm_initial)$adj.r.squared

# Perform Box-Cox transformation on the response variable
par(mfrow = c(1, 1))
box_initial_lm <- MASS::boxcox(
  object = lm_initial
  )

# identify the optimal lambda
optimal_lambda6 <- box_initial_lm$x[which.max(box_initial_lm$y)]
print(optimal_lambda6)
```

```{r }
# add transformed data to dataframe, use log of value if optimal lambda is 0
qol_data <- qol_data %>% 
  mutate(transformed_white = pct_white^optimal_lambda6)
```


```{r }
# Fit a new linear model with the transformed predictor variable
transformed_lm <- lm(response ~ transformed_white, qol_data)
summary(transformed_lm)
par(mfrow = c(2, 2))
plot(transformed_lm)

transformed_r_squared6 <- summary(transformed_lm)$r.squared
transformed_adj_r_squared6 <- summary(transformed_lm)$adj.r.squared

print(transformed_r_squared6)
print(transformed_adj_r_squared6)

# Store the results in the results data frame
# Store the results in the results data frame
temp_results <- rbind(results, data.frame(
  Predictor = "pct_white",
  Optimal_Lambda = optimal_lambda6,
  Original_R_squared = original_r_squared6,
  Original_Adj_R_squared = original_adj_r_squared6,
  Transformed_R_squared = transformed_r_squared6,
  Transformed_Adj_R_squared = transformed_adj_r_squared6
))

results <- temp_results
# Print the results
print(results)


```

*repeat*
```{r bc adult smokers}
# Fit the initial linear model
lm_initial <- lm(
  response ~ pct_adult_smokers,
  qol_data
  )
summary(lm_initial)

# Plot diagnostic plots for the initial model
par(mfrow = c(2, 2))
plot(lm_initial)

original_r_squared7 <- summary(lm_initial)$r.squared
original_adj_r_squared7 <- summary(lm_initial)$adj.r.squared

# Perform Box-Cox transformation on the response variable
par(mfrow = c(1, 1))
box_initial_lm <- MASS::boxcox(
  object = lm_initial
  )

# identify the optimal lambda
optimal_lambda7 <- box_initial_lm$x[which.max(box_initial_lm$y)]
print(optimal_lambda6)
```

```{r }
# add transformed data to dataframe, use log of value if optimal lambda is 0
qol_data <- qol_data %>% 
  mutate(transformed_smokers = pct_adult_smokers^optimal_lambda7)
```


```{r }
# Fit a new linear model with the transformed predictor variable
transformed_lm <- lm(response ~ transformed_smokers, qol_data)
summary(transformed_lm)
par(mfrow = c(2, 2))
plot(transformed_lm)

transformed_r_squared7 <- summary(transformed_lm)$r.squared
transformed_adj_r_squared7 <- summary(transformed_lm)$adj.r.squared

print(transformed_r_squared7)
print(transformed_adj_r_squared7)

# Store the results in the results data frame
# Store the results in the results data frame
temp_results <- rbind(results, data.frame(
  Predictor = "pct_adult_smokers",
  Optimal_Lambda = optimal_lambda7,
  Original_R_squared = original_r_squared7,
  Original_Adj_R_squared = original_adj_r_squared7,
  Transformed_R_squared = transformed_r_squared7,
  Transformed_Adj_R_squared = transformed_adj_r_squared6
))

results <- temp_results
# Print the results
print(results)


```



```{r}
# Assuming your data frame is named 'response'
# Write the data frame to a CSV file
write.csv(results, file = "results.csv", row.names = FALSE)


```










