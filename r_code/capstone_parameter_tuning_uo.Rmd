---
title: "Capstone Project Pre-Processing and Feature Engineering"
author: "Steve Uzupis"
date: "2024-07-07"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

$$Enviroment Build$$

*Packages Load*
```{r package load: su}
# load packages

library(readr)         # load csv files
library(readxl)        # load excel files
library(dplyr)         # data manipulation
library(lubridate)     # date & time manipulation
library(ggplot2)       # data visualization
library(tidyr)         # collection of statistical packages, packages loaded individually
library(corrplot)      # to visualize correlations
library(leaps)         # for subset selection
library(caret)         # test for correlation
library(car)           # for VIF
library(scales)        # for visualizing plots in %
library(forcats)       # ordering data frames
library(codebookr)     # adding appendix to r code
library(gtsummary)     # creating tables
library(cardx)         # to include statistic results
library(moments)       # to calculate skewness and kurtosis
library(VIM)           # to run K- Nearest Neighbour
library(pROC)          # to analyse and display reciever operating characteristics(ROC) curvees
library(randomForest)  # to impletement Random Forest algorithm
library(glmnet)
library(tidymodels)
library(recipes)
```

*Data load*
```{r data load: su}
# load data
sdoh_data <- read_csv("data/sdoh_data.csv")
dim(sdoh_data)

# remove unwanted features, create calculated feature, convert fips_code to data type matching chr_data
sdoh_data <- sdoh_data %>% 
  select("COUNTYFIPS", 
         "STATE", 
         "COUNTY", 
         "REGION", 
         "ACS_TOT_POP_WT", 
         "ACS_AVG_HH_SIZE", 
         "ACS_PCT_MALE", 
         "ACS_PCT_AIAN", 
         "ACS_PCT_ASIAN", 
         "ACS_PCT_BLACK", 
         "ACS_PCT_HISPANIC", 
         "ACS_PCT_OTHER_RACE", 
         "ACS_PCT_WHITE", 
         "ACS_PCT_CHILD_1FAM", 
         "ACS_PCT_CHILDREN_GRANDPARENT", 
         "ACS_PCT_GRANDP_RESPS_NO_P", 
         "ACS_PCT_GRANDP_RESPS_P",
         "ACS_PCT_HH_OTHER_COMP", 
         "ACS_PCT_HH_INTERNET",
         "ACS_PCT_EMPLOYED",
         "ACS_PCT_HH_INC_99999",
         "ACS_PCT_MEDICARE_ONLY",
         "AHRF_CLIN_NURSE_SPEC_RATE", 
         "AHRF_DENTISTS_RATE",
         "AHRF_PHYSICIAN_ASSIST_RATE",
         "AMFAR_MHFAC_RATE",
         "CEN_POPDENSITY_COUNTY",
         "NEPHTN_HEATIND_90",
         "SAIPE_MEDIAN_HH_INCOME",
         "POS_MEDIAN_DIST_ED",
         "POS_MEDIAN_DIST_PED_ICU",
         "POS_MEDIAN_DIST_CLINIC", 
         "POS_MEDIAN_DIST_ALC", 
  ) %>% 
  mutate(percent_grandparents_as_guardians = ACS_PCT_CHILDREN_GRANDPARENT * ((ACS_PCT_GRANDP_RESPS_P + ACS_PCT_GRANDP_RESPS_NO_P)/100)) %>% 
  select(-ACS_PCT_GRANDP_RESPS_P, -ACS_PCT_GRANDP_RESPS_NO_P, -ACS_PCT_CHILDREN_GRANDPARENT) %>% 
  rename("fips_code" = "COUNTYFIPS",
         "state" = "STATE",
         "county" = "COUNTY",
         "region" = "REGION",
         "weighted_population"  = "ACS_TOT_POP_WT",
         "average_hh_size" = "ACS_AVG_HH_SIZE",
         "pct_male"  = "ACS_PCT_MALE",
         "pct_native_american" = "ACS_PCT_AIAN",
         "pct_asian" = "ACS_PCT_ASIAN",
         "pct_black" = "ACS_PCT_BLACK",
         "pct_hispanic" = "ACS_PCT_HISPANIC",
         "pct_other_race" = "ACS_PCT_OTHER_RACE",
         "pct_white" = "ACS_PCT_WHITE",
         "pct_single_parent" = "ACS_PCT_CHILD_1FAM",
         "pct_hh_other_computer" = "ACS_PCT_HH_OTHER_COMP",
         "pct_hh_internet" = "ACS_PCT_HH_INTERNET",
         "pct_employed" = "ACS_PCT_EMPLOYED",
         "pct_hh_inc_99999" = "ACS_PCT_HH_INC_99999",            # renamed by mg
         "pct_w_medicare" = "ACS_PCT_MEDICARE_ONLY",
         "clinical_nurse_pt" = "AHRF_CLIN_NURSE_SPEC_RATE",
         "dentist_pt" = "AHRF_DENTISTS_RATE",
         "pa_pt" = "AHRF_PHYSICIAN_ASSIST_RATE",
         "mental_health_faciliy_pt" = "AMFAR_MHFAC_RATE",
         "population_density" = "CEN_POPDENSITY_COUNTY",
         "days_over_90_f" = "NEPHTN_HEATIND_90",
         "median_hh_income" = "SAIPE_MEDIAN_HH_INCOME",
         "median_er_dist" = "POS_MEDIAN_DIST_ED",
         "median_pediatric_icu_dist" = "POS_MEDIAN_DIST_PED_ICU",
         "median_health_clinic_dist" = "POS_MEDIAN_DIST_CLINIC",
         "median_drug_alcohol_care_dist" = "POS_MEDIAN_DIST_ALC"
    
  ) %>% 
  mutate(fips_code = as.numeric(fips_code))

chr_data <- read_csv("data/chr_data.csv", skip = 1)
dim(chr_data)
# remove unwanted features
# convert principal care providers from per 100,000 people to per 1,000 people to match other data

chr_data <- chr_data %>%
  select("fipscode",
         "v002_rawvalue",
         "v009_rawvalue",
         "v011_rawvalue",
         "v070_rawvalue",  
         "v049_rawvalue",
         "v085_rawvalue",
         "v168_rawvalue", 
         "v069_rawvalue",
         "v044_rawvalue", 
         "v140_rawvalue",
         "v125_rawvalue",
         "v124_rawvalue",
         "v136_other_data_1",
         "v136_other_data_2",
         "v137_rawvalue",
         "v147_rawvalue",
         "v139_rawvalue",
         "v177_rawvalue",
         "v153_rawvalue",
         "v053_rawvalue", 
         "v058_rawvalue", 
         "v004_rawvalue",
  ) %>% 
  mutate(pcp_pt = v004_rawvalue/100) %>% 
  select(-v004_rawvalue) %>% 
  rename("fips_code" = "fipscode",
         "pct_poor_to_fair_health" = "v002_rawvalue",
         "pct_adult_smokers" = "v009_rawvalue",
         "pct_obese_adults" = "v011_rawvalue",
         "pct_no_exercise" = "v070_rawvalue",
         "pct_binge_drinkers" = "v049_rawvalue",
         "pct_under_65_no_health_insurance" = "v085_rawvalue",
         "pct_highschool_diploma" = "v168_rawvalue",
         "pct_some_college" = "v069_rawvalue",
         "inequality_ratio" = "v044_rawvalue",
         "social_clubs_per_10k" = "v140_rawvalue",
         "air_polution_metric" =  "v125_rawvalue",
         "water_quality" = "v124_rawvalue",                          # renamed by mg
         "pct_high_housing_costs" = "v136_other_data_1",
         "pct_overcrowded_hh" = "v136_other_data_2",
         "pct_food_insecurities" = "v139_rawvalue",
         "pct_voters" = "v177_rawvalue",
         "pct_home_owner" = "v153_rawvalue",
         "pct_65_plus" = "v053_rawvalue",
         "pct_rural_population" = "v058_rawvalue",
         "life_expectancy_years" = "v147_rawvalue",
         "pct_30_min_plus_commute"  = "v137_rawvalue")

# full data sets are extremely large, initial dimension reduction was performed previously
```

*Combine datasets*

```{r qol dataset create and clean: su}
# Create and clean the qol_data dataset
qol_data <- sdoh_data %>%
  inner_join(chr_data, by = "fips_code") %>%
  mutate(response = ifelse(pct_poor_to_fair_health >= 0.154, "worse", "better")) %>%
  mutate(response = as.factor(response)) %>%
  #select(-pct_poor_to_fair_health) %>%              # keep until analysis has been performed
  mutate_at(vars(state, county, region), as.factor)   # convert characters to factors

```

$$Pre-processing and Feature Enginering$$

eliminate variables with no predictive value: fipscode, county and state, pct_poor_to_fair_health:
```{r}
qol_data <- qol_data %>% select(-c(fips_code,county,state,pct_poor_to_fair_health))
```



```{r data split}
calcSplitRatio <- function(p = NA, df) {
  ## @p  = the number of parameters. by default, if none are provided, the number of columns (predictors) in the dataset are used
  ## @df = the dataframe that will be used for the analysis
  
  ## If the number of parameters isn't supplied, set it to the number of features minus 1 for the target
  if(is.na(p)) {
    p <- ncol(df) -1   ## COMMENT HERE
  }
  
  ## Calculate the ideal number of testing set
  test_N <- (1/sqrt(p))*nrow(df)
  ## Turn that into a testing proportion
  test_prop <- round((1/sqrt(p))*nrow(df)/nrow(df), 2)
  ## And find the training proportion
  train_prop <- 1-test_prop
  
  ## Tell us the results!
  print(paste0("The ideal split ratio is ", train_prop, ":", test_prop, " training:testing"))
  
  ## Return the size of the training set
  return(train_prop)
}

calcSplitRatio(df = qol_data)
```

Split data intraining and testing sets, delete non useful features, input missing values

```{r impune missing data values}
# Find columns with missing data
colSums(is.na(qol_data))

# Delete rows with missing values in the "response" column
qol_data <- qol_data[!is.na(qol_data$response), ]
# delete column with large number of missing values 'pcp_pt'

qol_data <- qol_data[, !names(qol_data) %in% "pcp_pt"]

set.seed(112)
# Splitting the data into train and test sets


data_split <- initial_split(qol_data, strata = "response", prop = 0.86)

qol_train <- training(data_split)
qol_test  <- testing(data_split)


# Selecting only numeric columns for imputation
numeric_cols <- sapply(qol_train, is.numeric)
qol_train_numeric <- qol_train[, numeric_cols]
qol_test_numeric <- qol_test[, numeric_cols]

# Imputing missing values using mice
library(mice)

# Imputing missing values in the training set
qol_train_imputed <- mice(qol_train_numeric, m = 1, method = "cart",  # Using CART for imputation
                           maxit = 5)  # Maximum iterations for imputation

# Imputing missing values in the test set using the trained model
qol_test_imputed <- mice(qol_test_numeric, m = 1, method = "cart",
                           maxit = 5,
                           use.all = FALSE)  # Using only the training data for imputation

# Combining the imputed numeric columns back with the original data
qol_train <- cbind(qol_train[, !numeric_cols], complete(qol_train_imputed))
qol_test <- cbind(qol_test[, !numeric_cols], complete(qol_test_imputed))

sum(colSums(is.na(qol_test)))

```

Encode character Variables

```{r character clean-up}
# use count of areas (state and region) for features that are characters
# Create dummy variables for 'region' in training data
#set.seed(1)
#dummies_train <- dummyVars(~ region, data = qol_train, fullRank = FALSE)
#qol_train_encoded <- predict(dummies_train, newdata = qol_train)
#qol_train <- cbind(qol_train, qol_train_encoded) %>%
  #select(-region) # Remove the original 'region' column

#set.seed(1)
# Create dummy variables for 'region' in test data
#dummies_test <- dummyVars(~ region, data = qol_test, fullRank = FALSE)
#qol_test_encoded <- predict(dummies_test, newdata = qol_test)
#qol_test <- cbind(qol_test, qol_test_encoded) %>%
  #select(-region) # Remove the original 'region' column

#Encode response variable as 1(worse) and 0(better) 

#encode response variable in training set
#qol_train <- qol_train %>%
  #mutate(response = ifelse(response == "better", 0, 1))
#table(qol_train$response)
# Convert response variable to factor 
#qol_train$response <- as.factor(qol_train$response)



#encode response variable in testing set
#qol_test <- qol_test %>%
  #mutate(response = ifelse(response == "better", 0, 1))
#table(qol_test$response)
# Convert response variable to factor 
#qol_test$response <- as.factor(qol_test$response)




```



```{r}

set.seed(12)
# Create a recipe
response_recipe <- recipe(response ~ ., data = qol_train) %>%
  step_mutate(response = ifelse(response == "better", 0, 1)) %>%  # Encode 'response' as 1 and 0
  step_dummy(region, one_hot = TRUE) %>%  # One-hot encode the 'region' feature to include all 4 regions
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  #step_rm(region) %>%  # Remove the original 'region' column
  #step_rm(response) %>%  # Remove the original 'response' column
  prep(training = qol_train, retain = TRUE)

# Apply the recipe to the training and testing datasets
qol_train <- bake(response_recipe, new_data = qol_train)
qol_test <- bake(response_recipe, new_data = qol_test)

#show distribution of response variable in 
# For qol_train dataset
table(qol_train$response)

# For qol_test dataset
table(qol_test$response)


```




```{r box-cox}
#  use to identify which transformation to use

# use square root  + 3 (from q-q values) to get shapiro wilk p-value of around 0.5, if less than 0.05, it is not normally distributed

# Q-Q plotsare useful if greater than 5000 observations
```

```{r center and scale}

```

```{r pca analysis for dimensionality reduction}
# run a pca (DO this with 0, 1) or lda (prefered) on numeric data


```

```{r cross validate}

```



*Subset regression viability check*

```{r qol regsubsets selection: su}
#qol_numeric <- qol_data %>% 
  #select(-state,
         #-county,
         #-region,
         #-life_expectancy_years,
         #-fips_code)

# identify best model for response with regsubset, tests for linear models 
#qol_regfit_full <- regsubsets(response ~ ., qol_numeric,
                             #really.big = TRUE,
                             #nvmax = 48)
```

```{r identifying ideal # of variables: su}
#reg_fit_summary <- summary(qol_regfit_full)

# identifying ideal number of variables for each metric
#which.min(reg_fit_summary$rss)      # always selects for model with all predictors, over-fits to training data
#which.max(reg_fit_summary$adjr2)    # increases with additional predictors, susceptible to training error
#which.min(reg_fit_summary$cp)       # penalizes models with more predictors with unbiased measure of MSE
#which.min(reg_fit_summary$bic)      # like Cp, but includes penalty term log(n) in error so more error introduced with more predictors
```

```{r visualize regsubset data: su}
# create plots of each metric to visualize 

par(mfrow = c(2,2))

#plot(reg_fit_summary$rss,
     #xlab="Number of Variables",
     #ylab="RSS",
     #type="l")
#points(45, reg_fit_summary$rss[45], col="red",cex=1.5,pch =20)

#plot(reg_fit_summary$adjr2,
     #xlab = "Number of Variables",
     #ylab = "Adjusted R-squared",
     #type = "l")
#points(30, reg_fit_summary$adjr2[30], col="red",cex=1.5,pch =20)

#plot(reg_fit_summary$cp,
     #xlab = "Number of Variables",
     #ylab = "Cp",
     #type = "l")
#points(27, reg_fit_summary$cp[27], col="red",cex=1.5,pch =20)

#plot(reg_fit_summary$bic,
     #xlab = "Number of Variables",
     #ylab = "BIC",
     #type = "l")
#points (14, reg_fit_summary$bic[14], col = "red", cex = 1.5, pch = 20)



```


Build a simple OLS and RF model



```{r }


# Convert response variable to numeric
qol_train$response <- as.numeric(as.character(qol_train$response))

# --- OLS Model ---

# Fit the OLS model
ols_model <- lm(response ~ ., data = qol_train)

# Print OLS coefficients
print(summary(ols_model)$coefficients)

# Predict on the test set
predictions_ols <- predict(ols_model, newdata = qol_test)

# Evaluate the OLS model
confusion_matrix_ols <- table(qol_test$response, predictions_ols > 0.5)
accuracy_ols <- sum(diag(confusion_matrix_ols)) / sum(confusion_matrix_ols)
print(paste("Accuracy (OLS):", accuracy_ols))

# Calculate feature importance for OLS
feature_importance_ols <- abs(summary(ols_model)$coefficients[, "Estimate"])
importance_df_ols <- data.frame(Feature = names(feature_importance_ols), Importance = feature_importance_ols)
importance_df_ols <- importance_df_ols[order(-importance_df_ols$Importance), ]

# --- Random Forest Model ---

# Train a random forest model
rf_model <- randomForest(as.factor(response) ~ ., data = qol_train, ntree = 1000, na.action = na.omit)

# Predict on the test set
predictions_rf <- predict(rf_model, qol_test, type = "prob")[, 2]

# Evaluate the Random Forest model
confusion_matrix_rf <- table(qol_test$response, predictions_rf > 0.5)
accuracy_rf <- sum(diag(confusion_matrix_rf)) / sum(confusion_matrix_rf)
print(paste("Accuracy (RF):", accuracy_rf))

# --- ROC Curve (Both Models) ---

# Function to plot ROC curves
plot_roc <- function(predictions, model_name) {
  roc_obj <- roc(qol_test$response, predictions)
  auc_value <- auc(roc_obj)
  
  roc_df <- data.frame(
    tpr = roc_obj$sensitivities,
    fpr = 1 - roc_obj$specificities
  )
  
  ggplot(roc_df, aes(x = fpr, y = tpr)) +
    geom_line() +
    geom_abline(linetype = "dashed") +
    xlab("False Positive Rate") +
    ylab("True Positive Rate") +
    ggtitle(paste("ROC Curve (", model_name, ", AUC =", round(auc_value, 4), ")"))
}

# Plot ROC curves for both models
plot_roc(predictions_ols, "OLS")
plot_roc(predictions_rf, "Random Forest")

# --- Feature Importance Visualization (OLS) ---

# Select top 20 features
top_20_features_ols <- head(importance_df_ols, 20)

# Plot top 20 feature importance for OLS
ggplot(top_20_features_ols, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 20 Feature Importance (OLS)", x = "Feature", y = "Importance") +
  theme_minimal()

# --- Feature Importance Visualization (RF) ---

# Calculate feature importance for RF
feature_importance_rf <- importance(rf_model)
importance_df_rf <- data.frame(Feature = rownames(feature_importance_rf), Importance = feature_importance_rf[, 1])
importance_df_rf <- importance_df_rf[order(-importance_df_rf$Importance), ]

# Select top 20 features
top_20_features_rf <- head(importance_df_rf, 20)

# Plot top 20 feature importance for RF
ggplot(top_20_features_rf, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 20 Feature Importance (RF)", x = "Feature", y = "Importance") +
  theme_minimal()


```




Assumptions
OLS Model
Linearity: Assumes a linear relationship between the predictors and the response variable.
Independence: Assumes that the observations are independent.
Homoscedasticity: Assumes constant variance of the errors.
Normality: Assumes that the errors are normally distributed.
To check these assumptions,  use the following diagnostics:

Residuals Plot: To check for homoscedasticity and linearity.
Q-Q Plot: To check for normality of residuals.
Durbin-Watson Test: To check for independence of residuals.
```{r}

# Residuals plot
plot(ols_model, which = 1)

# Q-Q plot
plot(ols_model, which = 2)

# Durbin-Watson test
library(lmtest)
dwtest(ols_model)
```



build Lasso Model
```{r}



# Convert response variable to factor
qol_train$response <- as.factor(qol_train$response)
qol_test$response <- as.factor(qol_test$response)

# Create design matrices (excluding response variable)
x_train <- model.matrix(~ ., data = qol_train[,-which(names(qol_train) == "response")])
x_test <- model.matrix(~ ., data = qol_test[,-which(names(qol_test) == "response")])

# Fit the Lasso model
lasso_model <- glmnet(x_train, qol_train$response, family = "binomial", alpha = 1) 

# Find optimal lambda using cross-validation
cv_lasso <- cv.glmnet(x_train, qol_train$response, family = "binomial", alpha = 1)

# Predict on the test set using the optimal lambda
predictions_lasso <- predict(lasso_model, newx = x_test, s = cv_lasso$lambda.min, type = "response")

# Convert probabilities to class predictions (0 or 1)
predicted_classes_lasso <- ifelse(predictions_lasso > 0.5, 1, 0)

# Calculate accuracy
accuracy_lasso <- mean(qol_test$response == predicted_classes_lasso)
cat("Accuracy (Lasso):", accuracy_lasso, "\n")

# Plot ROC curve
roc_lasso <- roc(qol_test$response, predictions_lasso)
auc_lasso <- auc(roc_lasso)
cat("AUC (Lasso):", auc_lasso, "\n")

ggplot(data.frame(fpr = 1 - roc_lasso$specificities, tpr = roc_lasso$sensitivities), 
       aes(x = fpr, y = tpr)) +
  geom_line() +
  geom_abline(linetype = "dashed") +
  ggtitle(paste("ROC Curve (Lasso, AUC =", round(auc_lasso, 2), ")")) +
  xlab("False Positive Rate") +
  ylab("True Positive Rate")

# Feature importance

# Extract coefficients as a dense vector
coef_lasso <- coef(lasso_model, s = cv_lasso$lambda.min)

# Extract non-zero coefficients
non_zero_coefs <- coef_lasso[which(coef_lasso != 0)]

# Get the names of the non-zero coefficients
feature_names <- rownames(coef_lasso)[which(coef_lasso != 0)]

# Create the data frame
importance_df_lasso <- data.frame(Feature = feature_names, Importance = abs(non_zero_coefs))


importance_df_lasso <- importance_df_lasso[order(-importance_df_lasso$Importance), ]
top_20_features_lasso <- head(importance_df_lasso, 20)

# Plot top 20 feature importance
ggplot(top_20_features_lasso, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 20 Feature Importance (Lasso)", x = "Feature", y = "Importance") +
  theme_minimal()
```

